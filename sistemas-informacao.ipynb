{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Materiais e Métodos:\n",
    "\n",
    "- O objetivo inicial da pesquisa foi encontrar uma base de dados adequada para desenvolver um sistema de recomendação. Para isso, foi escolhida a base de dados \"Book-Crossing Dataset\", disponível na plataforma Kaggle, que contém informações sobre 271.379 usuários, 278.859 livros e suas avaliações, sendo que nem todos os usuários avaliaram todos os livros. \n",
    "- O sistema foi desenvolvido em Python, utilizando as bibliotecas Pandas, Scikit-learn e Kaggle, com o código sendo implementado no Jupyter Notebook. A recomendação foi realizada por meio de filtragem colaborativa com o algoritmo K-Nearest Neighbors (KNN), baseado nas classificações dos usuários."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: kagglehub in ./.local/lib/python3.13/site-packages (0.3.4)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3.13/site-packages (from kagglehub) (24.1)\n",
      "Requirement already satisfied: requests in /usr/lib/python3.13/site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/lib/python3.13/site-packages (from kagglehub) (4.66.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/lib/python3.13/site-packages (from requests->kagglehub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.13/site-packages (from requests->kagglehub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3.13/site-packages (from requests->kagglehub) (1.26.20)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.local/lib/python3.13/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/lib/python3.13/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.local/lib/python3.13/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.local/lib/python3.13/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in ./.local/lib/python3.13/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./.local/lib/python3.13/site-packages (from scikit-learn) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.local/lib/python3.13/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.local/lib/python3.13/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.local/lib/python3.13/site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in ./.local/lib/python3.13/site-packages (3.9.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.local/lib/python3.13/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.local/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.local/lib/python3.13/site-packages (from matplotlib) (4.55.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.local/lib/python3.13/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in ./.local/lib/python3.13/site-packages (from matplotlib) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3.13/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in ./.local/lib/python3.13/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.local/lib/python3.13/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/lib/python3.13/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: seaborn in ./.local/lib/python3.13/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in ./.local/lib/python3.13/site-packages (from seaborn) (2.1.3)\n",
      "Requirement already satisfied: pandas>=1.2 in ./.local/lib/python3.13/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in ./.local/lib/python3.13/site-packages (from seaborn) (3.9.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.local/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.local/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.local/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.55.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.local/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in ./.local/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.local/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/lib/python3.13/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.local/lib/python3.13/site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.local/lib/python3.13/site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install kagglehub\n",
    "%pip install pandas\n",
    "%pip install scikit-learn\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando o DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.read_csv(\"/home/cscarabelotti/Desktop/projects/sistemas-recomendacao/datasets/Books.csv\", sep=\";\", header=0)\n",
    "users = pd.read_csv(\"/home/cscarabelotti/Desktop/projects/sistemas-recomendacao/datasets/Users.csv\", sep=\";\", low_memory=False, header=0)\n",
    "ratings = pd.read_csv(\"/home/cscarabelotti/Desktop/projects/sistemas-recomendacao/datasets/Ratings.csv\", sep=\";\", low_memory=False, header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualização dos dados importados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books\n",
      "         ISBN                                              Title  \\\n",
      "0  0195153448                                Classical Mythology   \n",
      "1  0002005018                                       Clara Callan   \n",
      "2  0060973129                               Decision in Normandy   \n",
      "3  0374157065  Flu: The Story of the Great Influenza Pandemic...   \n",
      "4  0393045218                             The Mummies of Urumchi   \n",
      "\n",
      "                 Author  Year                Publisher  \n",
      "0    Mark P. O. Morford  2002  Oxford University Press  \n",
      "1  Richard Bruce Wright  2001    HarperFlamingo Canada  \n",
      "2          Carlo D'Este  1991          HarperPerennial  \n",
      "3      Gina Bari Kolata  1999     Farrar Straus Giroux  \n",
      "4       E. J. W. Barber  1999   W. W. Norton & Company   \n",
      "\n",
      "Informações sobre os livros\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 271379 entries, 0 to 271378\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   ISBN       271379 non-null  object\n",
      " 1   Title      271379 non-null  object\n",
      " 2   Author     271377 non-null  object\n",
      " 3   Year       271379 non-null  int64 \n",
      " 4   Publisher  271377 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 10.4+ MB\n",
      "None \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Books\")\n",
    "print(books.head(), \"\\n\")\n",
    "\n",
    "print(\"Informações sobre os livros\")\n",
    "print(books.info(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings\n",
      "   User-ID        ISBN  Rating\n",
      "0   276725  034545104X       0\n",
      "1   276726  0155061224       5\n",
      "2   276727  0446520802       0\n",
      "3   276729  052165615X       3\n",
      "4   276729  0521795028       6 \n",
      "\n",
      "Informações sobre os ratings\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1149780 entries, 0 to 1149779\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count    Dtype \n",
      "---  ------   --------------    ----- \n",
      " 0   User-ID  1149780 non-null  int64 \n",
      " 1   ISBN     1149780 non-null  object\n",
      " 2   Rating   1149780 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 26.3+ MB\n",
      "None \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Ratings\")\n",
    "print(ratings.head(), \"\\n\")\n",
    "\n",
    "print(\"Informações sobre os ratings\")\n",
    "print(ratings.info(), \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users\n",
      "  User-ID  Age\n",
      "0       1  NaN\n",
      "1       2   18\n",
      "2       3  NaN\n",
      "3       4   17\n",
      "4       5  NaN \n",
      "\n",
      "Informações sobre os usuários\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 278859 entries, 0 to 278858\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count   Dtype \n",
      "---  ------   --------------   ----- \n",
      " 0   User-ID  278859 non-null  object\n",
      " 1   Age      168627 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 4.3+ MB\n",
      "None \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Users\")\n",
    "print(users.head(), \"\\n\")\n",
    "\n",
    "print(\"Informações sobre os usuários\")\n",
    "print(users.info(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajustando os dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A base de dados é composta por três arquivos: books.csv, ratings.csv e users.csv. Em todos os arquivos, foram removidos valores duplicados, campos vazios e espaços extras nos dados de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Books\n",
    "books.dropna(subset=[\"Title\", \"Author\", \"Publisher\"], inplace=True)\n",
    "books[\"ISBN\"] = books[\"ISBN\"].str.strip()\n",
    "books[\"Title\"] = books[\"Title\"].str.strip()\n",
    "books[\"Author\"] = books[\"Author\"].str.strip()\n",
    "books[\"Publisher\"] = books[\"Publisher\"].str.strip()\n",
    "books.drop_duplicates(subset=[\"ISBN\"], inplace=True)\n",
    "\n",
    "books.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Em ratings, as avaliações duplicadas de um mesmo usuário para o mesmo livro foram eliminadas, mantendo a média das avaliações, e garantiu-se que as avaliações estivessem entre 0 e 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratings\n",
    "ratings.dropna(inplace=True)\n",
    "ratings[\"User-ID\"] = ratings[\"User-ID\"].astype(str).str.strip()\n",
    "ratings[\"ISBN\"] = ratings[\"ISBN\"].str.strip()\n",
    "ratings = ratings[(ratings[\"Rating\"] >= 0) & (ratings[\"Rating\"] <= 10)]\n",
    "ratings = ratings.groupby([\"User-ID\", \"ISBN\"], as_index=False)[\"Rating\"].mean()\n",
    "ratings.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Em users, as idades dos usuários foram convertidas para números, validadas entre 0 e 100, e dados inválidos foram descartados (como NaN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Users\n",
    "users[\"Age\"] = pd.to_numeric(users[\"Age\"], errors=\"coerce\")\n",
    "users = users[(users[\"Age\"] >= 0) & (users[\"Age\"] <= 100)]\n",
    "users.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separando os dados em treino/teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do treino: 18396\n",
      "Tamanho do teste: 4600\n"
     ]
    }
   ],
   "source": [
    "sampled_ratings = ratings.sample(frac=0.02, random_state=42)\n",
    "train, test = train_test_split(sampled_ratings, test_size=0.2, random_state=42)\n",
    "print(f\"Tamanho do treino: {len(train)}\")\n",
    "print(f\"Tamanho do teste: {len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando a matriz de iteração\n",
    "\n",
    "- É criada uma matriz de iteração, que relaciona usuários (identificados por ID) com livros (identificados pelo ISBN) e suas respectivas avaliações.\n",
    "\n",
    "- Nessa matriz, os livros são representados nas colunas, os usuários nas linhas, e as células contêm as avaliações atribuídas pelos usuários aos livros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_matrix = sampled_ratings.pivot(index='User-ID', columns='ISBN', values='Rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementação do Algoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculando a distância euclidiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_matrix_filled = rating_matrix.fillna(0)\n",
    "user_similarity = cosine_similarity(rating_matrix_filled)\n",
    "user_similarity_df = pd.DataFrame(user_similarity, index=rating_matrix_filled.index, columns=rating_matrix_filled.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizando recomendações\n",
    "\n",
    "- Após calcular a distância euclidiana entre os vizinhos, o algoritmo identifica os 10 usuários mais semelhantes ao usuário alvo. \n",
    "\n",
    "- Em seguida, seleciona os livros avaliados pelo usuário alvo e cria um dicionário vazio para armazenar as recomendações. \n",
    "\n",
    "- O algoritmo então verifica os livros avaliados pelos usuários vizinhos, adicionando ao dicionário aqueles que o usuário alvo ainda não avaliou. \n",
    "\n",
    "- Após concluir essa etapa, calcula-se a média das avaliações de cada livro recomendado, e a lista de recomendações é ordenada com base nessas médias, retornando os livros recomendados ao usuário alvo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wanderlust (Dragonlance: The Meetings Sextet, Vol. 2)',\n",
       " 'The Midnight Cafe (Anita Blake, Vampire Hunter)',\n",
       " 'Death of a Hussy',\n",
       " 'Zen and the Art of Motorcycle Maintenance: An Inquiry into Values',\n",
       " 'The Mists of Avalon']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def recommend_books(user_id, rating_matrix, user_similarity_df, books, top_n=5):\n",
    "\n",
    "    similar_users = user_similarity_df[user_id].drop(user_id).sort_values(ascending=False).head(10)\n",
    "    \n",
    "    books_rated_by_user = rating_matrix.loc[user_id].dropna().index\n",
    "    \n",
    "    book_recommendations = {}\n",
    "\n",
    "    for similar_user_id in similar_users.index:\n",
    "        books_rated_by_similar_user = rating_matrix.loc[similar_user_id].dropna()\n",
    "        \n",
    "        for book, rating in books_rated_by_similar_user.items():\n",
    "            if pd.isna(rating_matrix.loc[user_id, book]):\n",
    "                if book not in book_recommendations:\n",
    "                    book_recommendations[book] = []\n",
    "                book_recommendations[book].append(rating)\n",
    "    \n",
    "    average_ratings = {book: np.mean(ratings) for book, ratings in book_recommendations.items()}\n",
    "    \n",
    "    recommended_books = sorted(average_ratings.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    recommended_books_with_titles = []\n",
    "    for isbn, _ in recommended_books[:top_n]:\n",
    "        book_title = books[books['ISBN'] == isbn]['Title'].values[0] \n",
    "        recommended_books_with_titles.append(book_title)\n",
    "    \n",
    "    return recommended_books_with_titles\n",
    "\n",
    "recommended_books_list = recommend_books(\"100004\", rating_matrix, user_similarity_df, books, top_n=5)\n",
    "recommended_books_list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação do Algoritmo - Resultados e Conclusão\n",
    "\n",
    "- O sistema de recomendação foi desenvolvido utilizando o algoritmo KNN e filtragem colaborativa, baseado na base de dados “Book-Crossing Dataset”. \n",
    "\n",
    "- Durante o desenvolvimento, um desafio foi o alto consumo de recursos, tornando o processamento lento e custoso. \n",
    "\n",
    "- Além disso, a ausência de avaliações reais nos dados de treino e teste impediu uma análise mais completa, com a medição de acurácia ou precisão, por exemplo. \n",
    "\n",
    "- Apesar dessas limitações, a implementação do KNN foi bem-sucedida, criando um sistema básico de recomendação. \n",
    "\n",
    "- O projeto destacou a importância de um bom tratamento de dados para o bom funcionamento do modelo.\n",
    "\n",
    "- O tratamento adequado dos dados, como a remoção de duplicatas e transformação da idade dos usuários, contribuiu para o sucesso do funcionamento do modelo. \n",
    "\n",
    "- Para trabalhos futuros, uma possibilidade de melhorar o desenvolvimento atual é trazendo o foco para a coleta de dados de usuários reais, afim de que seja possível mensurar a acurácia e eficácia das recomendações geradas pelo sistema."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
